{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import einops\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import torch as t \n",
    "from torch import Tensor, from_numpy\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generatePatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/thomasbush/Documents/DSS_Tilburg/data/keyframes/_2025-04-22 00:25:47.432414_keyframes.pth'\n",
    "data = t.load(data_path, map_location='cpu', weights_only=False)\n",
    "keyframes = data['keyframes']\n",
    "idx = data['keyframe_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick script to display 8 keyframes \n",
    "idxs = t.randperm(keyframes.shape[0])[:8]\n",
    "selected_keyframes = keyframes[idxs]\n",
    "\n",
    "# Plot them in a 2x4 grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(selected_keyframes[i].cpu().numpy(), cmap='gray')\n",
    "    ax.set_title(f\"Keyframe {idxs[i].item()}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"selected_keyframes_grid.png\", dpi=300)  # Save if you want\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in idxs:\n",
    "    plt.figure()\n",
    "    plt.imshow(keyframes[i], cmap='gray')\n",
    "    plt.axis('Off')\n",
    "    plt.savefig(save_path / f\"keyframe{i}.svg\", format=\"svg\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = keyframes[100]\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "# plt.title('Original Frame')\n",
    "plt.axis('Off')\n",
    "plt.savefig(save_path / \"origina_frame.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_tensor, original_shape, padding  = generatePatches(img.unsqueeze(dim=0), dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_tensor = patches_tensor.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_patches(patches_tensor, original_shape, patch_dim, padding, save_path: None|Path):\n",
    "    \"\"\"\n",
    "    patches: tensor of shape (N, d, d) or (N, 1, d, d)\n",
    "    original_shape: tuple (H, W)\n",
    "    patch_dim: int\n",
    "    padding: tuple (top, bottom, left, right)\n",
    "    \"\"\"\n",
    "    H, W = original_shape\n",
    "    pad_top, pad_bottom, pad_left, pad_right = padding\n",
    "\n",
    "    # Total padded dimensions\n",
    "    H_p = H + pad_top + pad_bottom\n",
    "    W_p = W + pad_left + pad_right\n",
    "\n",
    "    # Convert patches to tensor if needed\n",
    "    if isinstance(patches_tensor, np.ndarray):\n",
    "        patches_tensor = t.tensor(patches_tensor)\n",
    "\n",
    "    # If patches have channels, remove it\n",
    "    if patches_tensor.dim() == 4 and patches_tensor.shape[1] == 1:\n",
    "        patches_tensor = patches_tensor.squeeze(1)\n",
    "\n",
    "    # Reconstruct the image from patches\n",
    "    num_patches_h = H_p // patch_dim\n",
    "    num_patches_w = W_p // patch_dim\n",
    "    assert patches_tensor.shape[0] == num_patches_h * num_patches_w\n",
    "\n",
    "    patches_grid = patches_tensor.view(num_patches_h, num_patches_w, patch_dim, patch_dim)\n",
    "    reconstructed = patches_grid.permute(0, 2, 1, 3).contiguous().view(H_p, W_p)\n",
    "\n",
    "    # Remove the padding to recover the original image\n",
    "    reconstructed = reconstructed[pad_top:pad_top+H, pad_left:pad_left+W]\n",
    "\n",
    "    # Plot the image\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(reconstructed, cmap='gray')\n",
    "    # ax.set_title(\"Image with Patch Boundaries\")\n",
    "\n",
    "    for i in range(0, H_p, patch_dim):\n",
    "        for j in range(0, W_p, patch_dim):\n",
    "            # Draw rectangles in padded space\n",
    "            y = i - pad_top\n",
    "            x = j - pad_left\n",
    "            rect = patches.Rectangle((x, y), patch_dim, patch_dim,\n",
    "                                            linewidth=0.5, edgecolor='blue', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path / \"patches_overlay.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('/Users/thomasbush/Documents/DSS_Tilburg/data/images')\n",
    "plot_image_with_patches(patches_tensor, (original_shape[1], original_shape[2]), 64, padding, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('/Users/thomasbush/Documents/DSS_Tilburg/data/images')\n",
    "plt.savefig(save_path / \"patches_overlay.svg\", format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(patches_tensor[24], cmap='gray')\n",
    "plt.axis('Off')\n",
    "plt.savefig(save_path / \"patch.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_n = einops.rearrange(patches_tensor[24], 'h w -> 1 1 h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyInverseCrf(img: t.Tensor, gamma: float = 2.2) -> t.Tensor:\n",
    "    return img ** gamma\n",
    "def addRealisticNoise(img: t.Tensor, sigma_s=0.12, sigma_c=0.03) -> t.Tensor:\n",
    "    \"\"\"\n",
    "    Realistic noise: L + n_s + n_c\n",
    "    img: (B, C, H, W), float32 in [0,1]\n",
    "    \"\"\"\n",
    "    # Inverse CRF (simulate raw sensor signal)\n",
    "    linear_img = applyInverseCrf(img)\n",
    "\n",
    "    # Signal-dependent noise (shot noise)\n",
    "    noise_s = t.randn_like(linear_img) * t.sqrt(t.clamp(linear_img * sigma_s**2, min=1e-6))\n",
    "\n",
    "    # Constant noise (read noise)``\n",
    "    noise_c = t.randn_like(linear_img) * sigma_c\n",
    "\n",
    "    noisy_img = linear_img + noise_s + noise_c\n",
    "    noisy_img = t.clamp(noisy_img, 0, 1)\n",
    "    return (noisy_img ** (1 / 2.2)).clamp(0, 1) * 1.5  # increase brightness/contrast\n",
    "\n",
    "\n",
    "img_noisy = addRealisticNoise(img_n, sigma_s=0.3, sigma_c=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(img_noisy.squeeze(dim=(0, 1)), cmap='gray')\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to Python path\n",
    "from utils import loadVideoArray, splitPadding\n",
    "from denoising.models import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(latent_dim_size=32, hidden_dim_size=128)\n",
    "x = t.randn((5, 1, 64, 64))\n",
    "\n",
    "y = model.forward(x)\n",
    "make_dot(y.mean(), params=dict(model.encoder.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/Users/thomasbush/Documents/Vault/Iurilli_lab/3d_tracking/data/multicam_video_2024-07-22T10_19_22_cropped_20250325101012/multicam_video_2024-07-22T10_19_22_mirror-left.avi.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to Python path\n",
    "from utils import loadVideoArray, splitPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = loadVideoArray(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.linspace(0, vid.shape[0]-1, 4).astype(int)\n",
    "frames = vid[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(frames[i], cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / \"frames_ex.png\", dpi=300)  # Save if you want\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstFrame(videofile):\n",
    "    vidcap = cv2.VideoCapture(videofile)\n",
    "    success, image = vidcap.read()\n",
    "    # if success:\n",
    "    #     cv2.imwrite(\"first_frame.jpg\", image)\n",
    "    return image\n",
    "full_path = '/Users/thomasbush/Documents/Vault/Iurilli_lab/3d_tracking/data/uncropped_cal/multicam_video_2024-07-24T14_13_45.avi'\n",
    "\n",
    "first_frame = getFirstFrame(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(first_frame, cmap='gray')\n",
    "plt.axis('Off')\n",
    "plt.savefig(save_path / 'fullimg.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_brightness = vid.mean(axis=(1,2))  # assuming shape (T, H, W)\n",
    "plt.plot(avg_brightness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import shannon_entropy\n",
    "entropies = [shannon_entropy(f) for f in vid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(entropies)\n",
    "plt.xlabel(\"Frame Index\")\n",
    "plt.ylabel(\"Shannon Entropy\")\n",
    "plt.title(\"Frame Entropy Over Time\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / 'entropyvid.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(vid.flatten(), bins=100, color='gray')\n",
    "plt.xlabel(\"Pixel Intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Pixel Intensity Distribution (All Frames)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / 'pixelinteistdist.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Frame Difference-Based Keyframe Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key frames are frames that represent transitons or new visual content in a video sequence. Thus, we are going to try to select frames where the pixel-level difference is the greatest\n",
    "\n",
    "We have decided to use a Histogram Difference (statistical change) that uses the comparison between distribution of pixel intensities:\n",
    "\n",
    "1. Convert img to hist\n",
    "2. compute hist diff: like L1 or cos sim\n",
    "3. Select the top-k frames with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know compute the hist \n",
    "def compute_histograms(frames: np.ndarray, bins: int = 64) -> np.ndarray:\n",
    "    n_frames = frames.shape[0]\n",
    "    \n",
    "    flat_vid = einops.rearrange(frames, 'frame h w -> frame (h w)')\n",
    "\n",
    "    histograms = []\n",
    "    for i in tqdm(range(n_frames), desc='Compute pixel distribution'):\n",
    "        hist, _ = np.histogram(flat_vid[i], bins=bins, range=(0, 255), density=True)\n",
    "        histograms.append(hist)\n",
    "\n",
    "    histograms = np.stack(histograms)  # shape: (n_frames, bins)\n",
    "    assert histograms.shape == (n_frames, bins)\n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = compute_histograms(video_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the diff \n",
    "def compute_cosine_diff(histograms: np.ndarray) -> np.ndarray:\n",
    "    # Normalize the histograms first (L2 norm)\n",
    "    norms = np.linalg.norm(histograms, axis=1, keepdims=True)\n",
    "    hist_norm = histograms / (norms + 1e-8)  # prevent divide by 0\n",
    "\n",
    "    # Shift histograms to compare frame t and t-1\n",
    "    h1 = hist_norm[1:]       # t = 1 to N-1\n",
    "    h0 = hist_norm[:-1]      # t = 0 to N-2\n",
    "\n",
    "    # Cosine similarity = dot product of normalized vectors\n",
    "    sim = np.sum(h1 * h0, axis=1)  # shape: (n_frames - 1,)\n",
    "    diff = 1 - sim  # cosine distance (1 = completely different)\n",
    "\n",
    "    return diff  # shape: (n_frames - 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = compute_cosine_diff(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_keyframes(differences:np.ndarray, k:int = 30):\n",
    "    idx = np.argsort(differences)[-k:]\n",
    "    return np.sort(idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = select_keyframes(cos_sim)\n",
    "keyframes = video_array[idx, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Features Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract features from our frames using a pre trained convolutional network: we obtain a vector with extracted features, then we cluster in the feature space (K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.cluster import KMeans\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "torch.device(device)\n",
    "video_array = vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "model = resnet50(weights= weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the last layer, don't need it\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1]).to(device)  # chop off the last FC layer\n",
    "\n",
    "model.eval()  # important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_custom = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((224, 224)),  # OR use T.Resize with aspect ratio preservation + padding\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],  \n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we extract the feautues:\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, frame_array, transform=None, target_transform=None):\n",
    "        self.frames = frame_array\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.frames.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.frames[idx]  # shape: [H, W]\n",
    "        \n",
    "        # Convert grayscale -> RGB by repeating across 3 channels\n",
    "        img_rgb = np.stack([img] * 3, axis=-1)  # shape: [H, W, 3]\n",
    "        \n",
    "        img_pil = Image.fromarray(img_rgb.astype(np.uint8))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img_pil)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = CustomDataset(video_array, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "data_loader = DataLoader(img_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce the features:\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for img in tqdm(data_loader, desc=f'Extracting features '):\n",
    "        img = img.to(device)\n",
    "        features.append(model.forward(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [img.to('cpu') for img in features]\n",
    "f = np.concatenate(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = f.mean(axis=(2, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the features for other analysis \n",
    "save_path_res = Path('/Users/thomasbush/Documents/DSS_Tilburg/data')\n",
    "\n",
    "np.save(save_path_res / 'features.npy',features_np , allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = .9)\n",
    "features_pca = pca.fit_transform(features_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=500, random_state=42)\n",
    "kmeans.fit(features_pca)\n",
    "\n",
    "closest_indices, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, features_pca)\n",
    "keyframe_indices = sorted(closest_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reducer = UMAP(n_components=2, random_state=42)\n",
    "embedding_2d = reducer.fit_transform(features_pca)  # shape: (N, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=labels, cmap='tab20', s=5, alpha=0.5, label=\"All frames\")\n",
    "keyframe_coords = embedding_2d[keyframe_indices]\n",
    "\n",
    "plt.scatter(keyframe_coords[:, 0], keyframe_coords[:, 1], \n",
    "            color='black', marker='*', s=10, label=\"Keyframes\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Keyframe Selection via K-Means Clustering\")\n",
    "plt.xlabel(\"UMAP Dim 1\")\n",
    "plt.ylabel(\"UMAP Dim 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "# Reduce dimensions for plotting\n",
    "save_path = Path('/Users/thomasbush/Documents/DSS_Tilburg/data/images')\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_np)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=500, random_state=42)\n",
    "kmeans.fit(features_pca)\n",
    "closest_indices, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, features_pca)\n",
    "\n",
    "# Assign clusters to all points\n",
    "labels = kmeans.predict(features_pca)\n",
    "colors = cm.tab20(labels.astype(float) % 20 / 20)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.scatter(features_pca[:, 0], features_pca[:, 1], c=colors, s=10, alpha=0.5, label=\"All frames\")\n",
    "plt.scatter(features_pca[closest_indices, 0], features_pca[closest_indices, 1],\n",
    "            c='black', s=60, marker='*', label=\"Keyframes\", edgecolors='white')\n",
    "\n",
    "# Annotate a few keyframe indices for draw.io use\n",
    "for idx in closest_indices[:20]:  # Only first 20 to avoid clutter\n",
    "    plt.annotate(str(idx), (features_pca[idx, 0], features_pca[idx, 1]), fontsize=8, color='black')\n",
    "\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.title(\"Keyframe Selection via KMeans Clustering\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / 'pca_kmean_clustering.svg', format=\"svg\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some of the frames in question:\n",
    "# quick script to display 8 keyframes \n",
    "idxs = closest_indices[:20]\n",
    "selected_keyframes = vid[idxs]\n",
    "\n",
    "# Plot them in a 2x4 grid\n",
    "fig, axes = plt.subplots(4, 5, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(selected_keyframes[i], cmap='gray')\n",
    "    ax.set_title(f\"Keyframe {idxs[i].item()}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / \"selected_keyframes_grid.svg\", format='svg', dpi=300) # Save if you want\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's als save them:\n",
    "import os \n",
    "os.makedirs(save_path / 'keyframes', exist_ok = True)\n",
    "\n",
    "for idx in tqdm(closest_indices[:20], desc='generating frames'):\n",
    "\n",
    "    plt.imsave(save_path / f'keyframes/keyframe{idx}.png', vid[idx],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubModLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Medoids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids = KMedoids(n_clusters=30, metric='euclidean', random_state=42)\n",
    "kmedoids.fit(features_pca)\n",
    "closest_indices_kmenoids, _ = pairwise_distances_argmin_min(kmedoids.cluster_centers_, features_pca)\n",
    "keyframe_indices = sorted(closest_indices_kmenoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "1. Coverage \n",
    "2. Redundancy Score\n",
    "3. Downstream Utility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Perceptual Hashing\n",
    "\n",
    "It generates a compact signature of an image based on its visual structure. The algorithm does:\n",
    "1. resize\n",
    "2. compute mena pixel intensity \n",
    "3. if p>m-> 1 else 0\n",
    "4. flatten the binary matrix inot a 1D bit string (for img= 8x8 -> 64 flatten)\n",
    "5. compares two hashes using Hamming distance=The Hamming distance between two equal-length strings of symbols is the number of positions at which the corresponding symbols are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsize = (16, 16)\n",
    "resized_frames = []\n",
    "\n",
    "for frame in tqdm(video_array):\n",
    "    resized = cv2.resize(frame, dsize=dsize, interpolation=cv2.INTER_LINEAR)\n",
    "    resized_frames.append(resized)\n",
    "\n",
    "resized_array = np.stack(resized_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute the mean intensity \n",
    "m = resized_array.mean(axis=(0,))\n",
    "# set to 1 if p > m else 0\n",
    "binary_img = np.where(resized_array>m, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we flatten our array:\n",
    "flatten_hashes = einops.rearrange(binary_img, 'f h w -> f (h w)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d_setup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
