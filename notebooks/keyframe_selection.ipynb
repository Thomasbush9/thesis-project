{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import einops\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/Users/thomasbush/Documents/Vault/Iurilli_lab/3d_tracking/data/multicam_video_2024-07-22T10_19_22_cropped_20250325101012/multicam_video_2024-07-22T10_19_22_mirror-bottom.avi.mp4\"\n",
    "/Users/thomasbush/Documents/DSS_Tilburg/data/keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to Python path\n",
    "from utils import loadVideoArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = loadVideoArray(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we load the video as frames\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# === Step 2: Initialize list to store frames ===\n",
    "frames = []\n",
    "\n",
    "# === Step 3: Read the video frame-by-frame ===\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # End of video\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # For grayscale\n",
    "    frames.append(frame)\n",
    "\n",
    "# === Step 4: Convert list to NumPy array ===\n",
    "video_array = np.array(frames)  # Shape: (num_frames, height, width, channels)\n",
    "\n",
    "# === Step 5: Cleanup ===\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Frame Difference-Based Keyframe Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key frames are frames that represent transitons or new visual content in a video sequence. Thus, we are going to try to select frames where the pixel-level difference is the greatest\n",
    "\n",
    "We have decided to use a Histogram Difference (statistical change) that uses the comparison between distribution of pixel intensities:\n",
    "\n",
    "1. Convert img to hist\n",
    "2. compute hist diff: like L1 or cos sim\n",
    "3. Select the top-k frames with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know compute the hist \n",
    "def compute_histograms(frames: np.ndarray, bins: int = 64) -> np.ndarray:\n",
    "    n_frames = frames.shape[0]\n",
    "    \n",
    "    flat_vid = einops.rearrange(frames, 'frame h w -> frame (h w)')\n",
    "\n",
    "    histograms = []\n",
    "    for i in tqdm(range(n_frames), desc='Compute pixel distribution'):\n",
    "        hist, _ = np.histogram(flat_vid[i], bins=bins, range=(0, 255), density=True)\n",
    "        histograms.append(hist)\n",
    "\n",
    "    histograms = np.stack(histograms)  # shape: (n_frames, bins)\n",
    "    assert histograms.shape == (n_frames, bins)\n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = compute_histograms(video_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the diff \n",
    "def compute_cosine_diff(histograms: np.ndarray) -> np.ndarray:\n",
    "    # Normalize the histograms first (L2 norm)\n",
    "    norms = np.linalg.norm(histograms, axis=1, keepdims=True)\n",
    "    hist_norm = histograms / (norms + 1e-8)  # prevent divide by 0\n",
    "\n",
    "    # Shift histograms to compare frame t and t-1\n",
    "    h1 = hist_norm[1:]       # t = 1 to N-1\n",
    "    h0 = hist_norm[:-1]      # t = 0 to N-2\n",
    "\n",
    "    # Cosine similarity = dot product of normalized vectors\n",
    "    sim = np.sum(h1 * h0, axis=1)  # shape: (n_frames - 1,)\n",
    "    diff = 1 - sim  # cosine distance (1 = completely different)\n",
    "\n",
    "    return diff  # shape: (n_frames - 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = compute_cosine_diff(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_keyframes(differences:np.ndarray, k:int = 30):\n",
    "    idx = np.argsort(differences)[-k:]\n",
    "    return np.sort(idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = select_keyframes(cos_sim)\n",
    "keyframes = video_array[idx, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Features Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract features from our frames using a pre trained convolutional network: we obtain a vector with extracted features, then we cluster in the feature space (K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.cluster import KMeans\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "model = resnet50(weights= weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the last layer, don't need it\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1]).to(device)  # chop off the last FC layer\n",
    "\n",
    "model.eval()  # important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_custom = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((224, 224)),  # OR use T.Resize with aspect ratio preservation + padding\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],  \n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we extract the feautues:\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, frame_array, transform=None, target_transform=None):\n",
    "        self.frames = frame_array\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.frames.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.frames[idx]  # shape: [H, W]\n",
    "        \n",
    "        # Convert grayscale -> RGB by repeating across 3 channels\n",
    "        img_rgb = np.stack([img] * 3, axis=-1)  # shape: [H, W, 3]\n",
    "        \n",
    "        img_pil = Image.fromarray(img_rgb.astype(np.uint8))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img_pil)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = CustomDataset(video_array, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_loader = DataLoader(img_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce the features:\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for img in tqdm(data_loader, desc=f'Extracting features '):\n",
    "        img = img.to(device)\n",
    "        features.append(model.forward(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [img.to('cpu') for img in features]\n",
    "f = np.concatenate(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = f.mean(axis=(2, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = .9)\n",
    "features_pca = pca.fit_transform(features_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=30, random_state=42)\n",
    "kmeans.fit(features_pca)\n",
    "\n",
    "closest_indices, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, features_pca)\n",
    "keyframe_indices = sorted(closest_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some of the keyframes\n",
    "n_rows, n_cols = 3, 5  # 3x5 grid to show 15 frames\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Display up to 15 keyframes\n",
    "for i, ax in enumerate(axs):\n",
    "    if i < len(keyframe_indices):\n",
    "        ax.imshow(video_array[keyframe_indices[i]], cmap='gray')\n",
    "        ax.set_title(f'Frame {keyframe_indices[i]}')\n",
    "    ax.axis('off')  # Hide axes\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle(\"Keyframes Selected by K-Means\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Medoids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids = KMedoids(n_clusters=30, metric='euclidean', random_state=42)\n",
    "kmedoids.fit(features_pca)\n",
    "closest_indices_kmenoids, _ = pairwise_distances_argmin_min(kmedoids.cluster_centers_, features_pca)\n",
    "keyframe_indices = sorted(closest_indices_kmenoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "1. Coverage \n",
    "2. Redundancy Score\n",
    "3. Downstream Utility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Perceptual Hashing\n",
    "\n",
    "It generates a compact signature of an image based on its visual structure. The algorithm does:\n",
    "1. resize\n",
    "2. compute mena pixel intensity \n",
    "3. if p>m-> 1 else 0\n",
    "4. flatten the binary matrix inot a 1D bit string (for img= 8x8 -> 64 flatten)\n",
    "5. compares two hashes using Hamming distance=The Hamming distance between two equal-length strings of symbols is the number of positions at which the corresponding symbols are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsize = (16, 16)\n",
    "resized_frames = []\n",
    "\n",
    "for frame in tqdm(video_array):\n",
    "    resized = cv2.resize(frame, dsize=dsize, interpolation=cv2.INTER_LINEAR)\n",
    "    resized_frames.append(resized)\n",
    "\n",
    "resized_array = np.stack(resized_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute the mean intensity \n",
    "m = resized_array.mean(axis=(0,))\n",
    "# set to 1 if p > m else 0\n",
    "binary_img = np.where(resized_array>m, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we flatten our array:\n",
    "flatten_hashes = einops.rearrange(binary_img, 'f h w -> f (h w)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d_setup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
